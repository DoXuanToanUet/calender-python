import requests
from bs4 import BeautifulSoup
import json
import re
import time

# Äá»c danh sÃ¡ch URL tá»« file urls.txt
with open('urls.txt', 'r', encoding='utf-8') as f:
    urls = [line.strip() for line in f if line.strip()]

results = []
BATCH_SIZE = 100  # Má»—i batch cÃ³ 100 URL
success_count = 0
fail_count = 0
failed_urls = []

total_urls = len(urls)
start_time = time.time()  # Báº¯t Ä‘áº§u Ä‘áº¿m thá»i gian

for i in range(0, total_urls, BATCH_SIZE):
    batch = urls[i:i + BATCH_SIZE]
    
    print(f"\nğŸ”„ Äang crawl batch {i // BATCH_SIZE + 1}/{(total_urls // BATCH_SIZE) + 1} ({i+1}-{min(i+BATCH_SIZE, total_urls)}/{total_urls})...\n")
    
    for index, url in enumerate(batch, start=i+1):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()  # Kiá»ƒm tra lá»—i HTTP náº¿u cÃ³
            soup = BeautifulSoup(response.text, "html.parser")

            # TÃ¬m tháº» div chá»©a dá»¯ liá»‡u (dá»±a vÃ o class "lcghn bgwhite")
            container = soup.find("div", class_="lcghn")
            if not container:
                print(f"âŒ KhÃ´ng tÃ¬m tháº¥y container: {url}")
                fail_count += 1
                failed_urls.append(url)
                continue

            # TrÃ­ch xuáº¥t ngÃ y tá»« URL (VD: "suy-niem-loi-chua-ngay-01-01-2025" -> "01-01-2025")
            match = re.search(r'(\d{2}-\d{2}-\d{4})', url)
            date = match.group(1) if match else "KhÃ´ng rÃµ"

            # Láº¥y ná»™i dung title tá»« tháº» h2
            title = container.find("h2").get_text(strip=True) if container.find("h2") else ""

            # Láº¥y ná»™i dung mÃ´ táº£ (desc) tá»« pháº§n tá»­ cÃ³ class "ttlcghn"
            desc = container.find(class_="ttlcghn").get_text(strip=True) if container.find(class_="ttlcghn") else ""

            # Láº¥y ná»™i dung bÃ i Ä‘á»c (read) tá»« pháº§n tá»­ cÃ³ class "dsbd"
            read_tag = container.find(class_="dsbd")
            if read_tag:
                read = read_tag.get_text("\n", strip=True)  # Láº¥y ná»™i dung
                read = re.sub(r'^.*?CÃ¡c bÃ i Ä‘á»c vÃ  tin má»«ng hÃ´m nay', '', read, flags=re.DOTALL).strip()  # Bá» pháº§n tiÃªu Ä‘á»
            else:
                read = ""

            # Láº¥y ná»™i dung Ã¡o lá»… (paole) tá»« pháº§n tá»­ cÃ³ class "paole"
            paole = container.find(class_="paole").get_text(strip=True) if container.find(class_="paole") else ""

            # ThÃªm dá»¯ liá»‡u vÃ o danh sÃ¡ch káº¿t quáº£
            results.append({
                "date": date,
                "title": title,
                "desc": desc,
                "read": read,
                "paole": paole
            })

            success_count += 1
            print(f"âœ… {index}/{total_urls} - ÄÃ£ crawl thÃ nh cÃ´ng: {date}")

        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Lá»—i request {url}: {e}")
            fail_count += 1
            failed_urls.append(url)
        except Exception as e:
            print(f"âš ï¸ Lá»—i khÃ¡c {url}: {e}")
            fail_count += 1
            failed_urls.append(url)

        time.sleep(0.5)  # Nghá»‰ 0.5 giÃ¢y giá»¯a cÃ¡c request Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n

# Ghi káº¿t quáº£ ra file JSON
with open("lich_cong_giao_2025.json", "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

# Ghi danh sÃ¡ch URL tháº¥t báº¡i ra file
if failed_urls:
    with open("failed_urls.txt", "w", encoding="utf-8") as f:
        for url in failed_urls:
            f.write(url + "\n")

# Káº¿t thÃºc Ä‘áº¿m thá»i gian
end_time = time.time()
total_time = end_time - start_time  # Tá»•ng thá»i gian crawl

# Thá»‘ng kÃª káº¿t quáº£ cuá»‘i cÃ¹ng
print("\nğŸ“Š **Tá»”NG Káº¾T:**")
print(f"âœ… ThÃ nh cÃ´ng: {success_count}/{total_urls}")
print(f"âŒ Tháº¥t báº¡i: {fail_count}/{total_urls}")
print(f"â³ Tá»•ng thá»i gian crawl: {total_time:.2f} giÃ¢y ({total_time/60:.2f} phÃºt)")
if fail_count > 0:
    print(f"ğŸ“ Danh sÃ¡ch URL tháº¥t báº¡i Ä‘Ã£ lÆ°u vÃ o 'failed_urls.txt'")

print("\nğŸ‰ HoÃ n thÃ nh viá»‡c crawl vÃ  lÆ°u dá»¯ liá»‡u vÃ o 'lich_cong_giao_2025.json'")
